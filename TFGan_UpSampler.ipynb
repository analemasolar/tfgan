{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8z4EC5v23AN"
      },
      "source": [
        "# Upsampling's tfgan experiment notebook\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtOuGKfYnknK"
      },
      "source": [
        "### Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOeCVWAan0RJ",
        "outputId": "0823e8b7-29a9-4a16-9fb6-a0bf5e4813f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "drive.mount('/content/drive')\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/TFG/UTK_Corto.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7LeESSawpzf"
      },
      "outputs": [],
      "source": [
        "%pip install comet_ml\n",
        "from comet_ml import Experiment\n",
        "from comet_ml.integration.pytorch import log_model\n",
        "experiment = Experiment(\n",
        "  api_key = \"g208he4vUVLoAY4sFmdPGDBDO\",\n",
        "  project_name = \"MonitorizaciÃ³n Final\",\n",
        "  workspace=\"kpstufforigins\",\n",
        "  log_code=True,\n",
        "  log_graph=True,\n",
        ")\n",
        "experiment.add_tag(\"TFGAN Up 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGIBEimcnjg8"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import wasserstein_distance\n",
        "import calendar\n",
        "import time as t\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from time import time\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import log2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from math import log2\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE_GEN = 1e-4\n",
        "LEARNING_RATE_DIS = 1e-4\n",
        "LAMBDA_IDENTITY = 0\n",
        "LAMBDA_CYCLE = 10\n",
        "NUM_WORKERS = 2\n",
        "NUM_EPOCHS = 1500\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = True\n",
        "CHECKPOINT_GEN_H = \"genh.pth.tar\"\n",
        "CHECKPOINT_GEN_Z = \"genz.pth.tar\"\n",
        "CHECKPOINT_CRITIC_H = \"critich.pth.tar\"\n",
        "CHECKPOINT_CRITIC_Z = \"criticz.pth.tar\"\n",
        "\n",
        "hyper_params = {\n",
        "   \"learning_rate_gen\": LEARNING_RATE_GEN,\n",
        "   \"learning_rate_dis\": LEARNING_RATE_DIS,\n",
        "   \"EPOCH\": NUM_EPOCHS,\n",
        "   \"batch_size\": BATCH_SIZE,\n",
        "   \"lambda_identity\": LAMBDA_IDENTITY\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yEFcpVfnoiC"
      },
      "source": [
        "### ARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVEd5IyM2yJv",
        "outputId": "2916d4dc-8e62-49a9-ba8b-d7ba04b2bc7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape returned by model: torch.Size([2, 3, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1), # Pads the input tensor using the reflection of the input boundary\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "\n",
        "\n",
        "class Generator_TFGan_Up_Modelo_1(nn.Module):\n",
        "\n",
        "    #Model 1 do not downsample the image, it multiplex the channels\n",
        "    #then apply the residuals blocks and directly upsample\n",
        "\n",
        "    def __init__(self, img_resolution, init_channels=3, frontier=3, num_residuals=3, rabit_hole=2):\n",
        "        \"\"\"\n",
        "        -> frontier es el numero de multiplexaciones iniciales\n",
        "        -> num_residuals es el numero de bloques residuales\n",
        "        -> rabit_hole es cuantos pasos de reescalamiento hay al final\n",
        "\n",
        "        Por default las imagenes se reescalan 2 veces, es decir, la resolucion final sera 4*img_resolution\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.img_resolution = img_resolution\n",
        "        self.frontier = frontier\n",
        "        self.num_residuals = num_residuals\n",
        "        current_size = img_resolution\n",
        "        self.init_channels = init_channels\n",
        "\n",
        "        #Initial Layers, Extraordinary upsampling\n",
        "        out_features = 64\n",
        "        self.model = [\n",
        "            nn.Conv2d(init_channels, out_features, kernel_size=3,stride=1,padding=1),\n",
        "            nn.InstanceNorm2d(out_features),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "        in_features = out_features\n",
        "\n",
        "        #Llegada a la frontera\n",
        "        for _ in range(frontier):\n",
        "            out_features *= 2\n",
        "            self.model += [\n",
        "              nn.Conv2d(in_features, out_features, 3, stride=1, padding=1), # --> width and height constant\n",
        "              nn.InstanceNorm2d(out_features),\n",
        "              nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "\n",
        "        #RESIDUAL\n",
        "        for _ in range(num_residuals):\n",
        "            self.model += [ResidualBlock(out_features)]\n",
        "\n",
        "        #UPSAMPLING\n",
        "        for _ in range(rabit_hole):\n",
        "            out_features //= 2\n",
        "            self.model += [\n",
        "                nn.Upsample(scale_factor=2), # --> width*2, heigh*2\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            current_size *= 2\n",
        "\n",
        "        # Output Layer\n",
        "        self.model += [\n",
        "                  nn.Conv2d(out_features, init_channels, 3,stride=1,padding=1),\n",
        "                  nn.Tanh()\n",
        "                 ]\n",
        "\n",
        "        #UNPACK\n",
        "        self.model_sequential = nn.Sequential(*self.model)\n",
        "\n",
        "        return\n",
        "\n",
        "    def forward(self,x):\n",
        "        #print(f\"init shape{x.shape}\")\n",
        "        for layer in self.model:\n",
        "            x = layer(x)\n",
        "        #print(f\"for layer:{layer} shape returned is: {x.shape}\")\n",
        "        return x\n",
        "        #return self.model_sequential(x)\n",
        "\n",
        "def test():\n",
        "    x = torch.randn((2,3,128,128))\n",
        "    model = Generator_TFGan_Up_Modelo_1(128)\n",
        "    #print(model)\n",
        "    print(f\"shape returned by model: {model(x).shape}\")\n",
        "    return\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR5in-RhoNp8",
        "outputId": "c22be0ab-15cf-4b65-dee0-3e394d18334d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img_size: 64\n",
            "tensor([[[[-0.2721, -0.0638,  0.6127, -0.7428,  0.1489, -0.0569],\n",
            "          [ 0.4339,  0.3629, -0.3181,  0.0514,  0.1529, -0.1327],\n",
            "          [-0.2702, -0.4321, -0.4469, -0.4032, -0.5285,  0.5252],\n",
            "          [ 0.1467, -0.3046,  0.2535, -0.0860, -0.7569, -0.6893],\n",
            "          [ 0.2383,  0.5676,  0.4166, -1.0149, -0.0142, -0.0887],\n",
            "          [-0.1984, -0.1738,  0.7834, -0.2528, -0.1990,  0.0903]]]],\n",
            "       device='cuda:0', grad_fn=<ConvolutionBackward0>)\n",
            "torch.Size([1, 1, 6, 6])\n"
          ]
        }
      ],
      "source": [
        "class Discriminador(nn.Module):\n",
        "    def __init__(self, channels = 3):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        nc = channels\n",
        "        ndf = 64\n",
        "        self.layers = nn.Sequential(\n",
        "            # input is (nc) x 128 x 128\n",
        "            nn.Conv2d(nc,ndf,4,2,1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 64 x 64\n",
        "            nn.Conv2d(ndf,ndf*2,4,2,1, bias=False),\n",
        "            nn.InstanceNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 32 x 32\n",
        "            nn.Conv2d(ndf*2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.InstanceNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            #ResidualBlock(ndf*4),\n",
        "            #ResidualBlock(ndf*4),\n",
        "\n",
        "            # state size. (ndf*4) x 16 x 16\n",
        "            nn.Conv2d(ndf*4,ndf*8,4,1,1),\n",
        "            nn.InstanceNorm2d(ndf*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 15 x 15\n",
        "            nn.Conv2d(ndf*8,1,4,1,1)\n",
        "            # state size. 1 x 14 x 14\n",
        "        )\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      #print(f\"input shape: {x.shape}\")\n",
        "      return self.layers(x)\n",
        "\n",
        "def test(i):\n",
        "    img_size = i\n",
        "    model = Discriminador().to(DEVICE)\n",
        "\n",
        "    print(f\"img_size: {img_size}\")\n",
        "    x = torch.randn((1,3,img_size,img_size)).to(DEVICE)\n",
        "\n",
        "    print(model(x))\n",
        "    #print(torch.mean(model(x)))\n",
        "    print(model(x).shape)\n",
        "    #print(model)\n",
        "    return\n",
        "\n",
        "test(64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-YiLZ1Hoy9u"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmjBmnl4o02G"
      },
      "outputs": [],
      "source": [
        "transforms1 = A.Compose(\n",
        "    [\n",
        "        A.Resize(width=32, height=32),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Normalize(mean=[0.5 for _ in range(3)], std=[0.5 for _ in range(3)], max_pixel_value=255),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    additional_targets={\"image0\": \"image\"},\n",
        ")\n",
        "\n",
        "transforms2 = A.Compose(\n",
        "    [\n",
        "        A.Resize(width=16, height=16),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Normalize(mean=[0.5 for _ in range(3)], std=[0.5 for _ in range(3)], max_pixel_value=255),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    additional_targets={\"image0\": \"image\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4uDAIXio7uO"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class TFGAN_Dataset(Dataset):\n",
        "\n",
        "    #Por estandar\n",
        "    #  root_A = Adultos\n",
        "    #  root_b = Kids\n",
        "\n",
        "    def __init__(self, root_A, root_B, transform1=None, transform2=None, gray_scale=True):\n",
        "        self.root_A = root_A\n",
        "        self.root_B = root_B\n",
        "        self.transform1 = transform1\n",
        "        self.transform2 = transform2\n",
        "        self.gray_scale = gray_scale\n",
        "\n",
        "        self.root_A_images = os.listdir(root_A)\n",
        "        self.root_B_images = os.listdir(root_B)\n",
        "        self.length_dataset = max(len(self.root_A_images), len(self.root_B_images))\n",
        "        self.root_A_len = len(self.root_A_images)\n",
        "        self.root_B_len = len(self.root_B_images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length_dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        A_image = self.root_A_images[index % self.root_A_len]\n",
        "        B_image = self.root_B_images[index % self.root_B_len]\n",
        "\n",
        "        A_path = os.path.join(self.root_A, A_image)\n",
        "        B_path = os.path.join(self.root_B, B_image)\n",
        "\n",
        "        A_image = np.array(Image.open(A_path).convert(\"L\" if self.gray_scale else \"RGB\"))\n",
        "        B_image = np.array(Image.open(B_path).convert(\"L\" if self.gray_scale else \"RGB\"))\n",
        "\n",
        "        A_image1 = A_image\n",
        "        A_image2 = A_image\n",
        "\n",
        "        B_image1 = B_image\n",
        "        B_image2 = B_image\n",
        "\n",
        "        if self.transform1:\n",
        "            augmentations1 = self.transform1(image=A_image, image0=B_image)\n",
        "            augmentations2 = self.transform2(image=A_image, image0=B_image)\n",
        "\n",
        "            A_image1 = augmentations1[\"image\"]\n",
        "            A_image2 = augmentations2[\"image\"]\n",
        "            B_image1 = augmentations1[\"image0\"]\n",
        "            B_image2 = augmentations2[\"image0\"]\n",
        "\n",
        "\n",
        "        return A_image1, B_image1, A_image2, B_image2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEU8GJLOpJqG"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cf9aa59"
      },
      "outputs": [],
      "source": [
        "def loss_gan(fake,real):\n",
        "    MSE = nn.L1Loss()\n",
        "    return MSE(fake,real)\n",
        "\n",
        "def loss_dis(fake, real):\n",
        "    MSE = nn.MSELoss()\n",
        "    return MSE(real,fake)\n",
        "\n",
        "def loss_id(in1, out1, in2, out2):\n",
        "    return torch.abs(in1-out1).mean() + torch.abs(in2-out2).mean()\n",
        "\n",
        "def dis_loss_wasserstein_gan(fake_img, real_img):\n",
        "    return torch.mean(real_img)-torch.mean(fake_img)\n",
        "\n",
        "def gen_loss_wasserstein_gan(fake_img): #fake_img es gen(input)\n",
        "    return torch.mean(fake_img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af61b1f2"
      },
      "outputs": [],
      "source": [
        "### Checkpoints\n",
        "import random, torch, os, numpy as np\n",
        "import copy\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    #print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFNxzJBwx_dV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def train_individual_down(\n",
        "    writer, epoch, i,\n",
        "    loader, l1, mse,\n",
        "    disc_Adults_down, disc_Kids_down, down_sampler, opt_disc_Adults_down, opt_disc_Kids_down, opt_gen_down, d_down_scaler, g_down_scaler,\n",
        "):\n",
        "\n",
        "    adults_reals = 0\n",
        "    adults_fakes = 0\n",
        "\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for idx, (adult, kid, adult_frontier, kid_frontier) in enumerate(loop):\n",
        "        i+=1\n",
        "        #Mandamos las cosas a su sitio\n",
        "        adult = adult.to(DEVICE)\n",
        "        adult_frontier = adult_frontier.to(DEVICE)\n",
        "        kid = kid.to(DEVICE)\n",
        "        kid_frontier = kid_frontier.to(DEVICE)\n",
        "\n",
        "        #Entrenamos los discriminadores\n",
        "        with torch.cuda.amp.autocast():\n",
        "            fake_adult_down = down_sampler(kid)\n",
        "\n",
        "            #DOWN\n",
        "            disc_adult_down_real = disc_Adults_down(adult_frontier)\n",
        "            disc_adult_down_fake = disc_Adults_down(fake_adult_down.detach())\n",
        "\n",
        "            disc_adult_down_loss = torch.mean(disc_adult_down_real) - torch.mean(disc_adult_down_fake)\n",
        "            writer.add_scalar(\"DisLoss\", disc_adult_down_loss,  i)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #Como no hay consistencia de ciclo no neecesitamos entrenar los segundos generadores\n",
        "            #Lo juntamos todo, no nos sirve para entreno solo para ver como va la cosa\n",
        "\n",
        "            D_loss = disc_adult_down_loss\n",
        "\n",
        "\n",
        "        #APLICAMOS LOS CAMBIOS DEL ENTRENO:\n",
        "\n",
        "        opt_disc_Adults_down.zero_grad()\n",
        "        d_down_scaler.scale(D_loss).backward()\n",
        "        d_down_scaler.step(opt_disc_Adults_down)\n",
        "        d_down_scaler.update()\n",
        "\n",
        "\n",
        "        #Ahora entrenamos el generador  con lo que hemos sacado de los discriminadores\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            generated_images = down_sampler(kid)\n",
        "\n",
        "            #adversarial loss\n",
        "            dis_adult_down_fake = disc_Adults_down(generated_images)\n",
        "            loss_gen_down_adult = torch.mean(dis_adult_down_fake)\n",
        "            writer.add_scalar(\"GenLoss\", loss_gen_down_adult, i)\n",
        "\n",
        "\n",
        "            #Se suma todo\n",
        "            g_down_loss = loss_gen_down_adult\n",
        "\n",
        "        #APLICAMOS LOS CAMBIOS DEL ENTRENO:\n",
        "\n",
        "        #DOWN\n",
        "        opt_gen_down.zero_grad()\n",
        "        g_down_scaler.scale(g_down_loss).backward()\n",
        "        g_down_scaler.step(opt_gen_down)\n",
        "        g_down_scaler.update()\n",
        "\n",
        "\n",
        "        if idx==0:\n",
        "            timestamp = t.time()\n",
        "            save_image(generated_images*0.5+0.5,f\"/content/drive/MyDrive/TFG/UpSamplerModelo1/output{timestamp}.png\")\n",
        "            save_image(kid*0.5+0.5,f\"/content/drive/MyDrive/TFG/UpSamplerModelo1/input{timestamp}.png\")\n",
        "            save_checkpoint(down_sampler, opt_gen_down, filename=\"/content/drive/MyDrive/TFG/UpSamplerModelo1/gen_down.pth.tar\")\n",
        "            save_checkpoint(disc_Kids_down, opt_disc_Adults_down, filename=\"/content/drive/MyDrive/TFG/UpSamplerModelo1/disc_Kids_down.pth.tar\")\n",
        "            save_checkpoint(disc_Adults_down, opt_disc_Kids_down, filename=\"/content/drive/MyDrive/TFG/UpSamplerModelo1/disc_Adults_down.pth.tar\")\n",
        "\n",
        "\n",
        "            img_grid = torchvision.utils.make_grid(adult)\n",
        "            #matplotlib_imshow(img_grid, one_channel=True)\n",
        "\n",
        "            writer.add_image('input_down', torchvision.utils.make_grid(kid), i)\n",
        "\n",
        "            writer.add_image('output_down', torchvision.utils.make_grid(fake_adult_down), i)\n",
        "            #writer.add_image('fake_adults_down', torchvision.utils.make_grid(fake_adult_down),idx)\n",
        "            #writer.add_image('fake_adults_res', torchvision.utils.make_grid(fake_adult_res), idx)\n",
        "            #writer.add_graph(down_sampler,kid)\n",
        "\n",
        "        loop.set_postfix(GenLoss = torch.mean(g_down_loss), DisLoss = torch.mean(D_loss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxEalX6xsUb7"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "09f6cd14"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "\n",
        "    #Dataset\n",
        "    img_size = 64\n",
        "    img_size_discriminator = 64*4\n",
        "    channels = 3\n",
        "\n",
        "    dataset_full = A.Compose([\n",
        "        A.Resize(width=img_size, height=img_size),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Normalize(mean=[0.5 for _ in range(channels)], std=[0.5 for _ in range(channels)], max_pixel_value=255),\n",
        "        ToTensorV2(),\n",
        "    ],additional_targets={\"image0\": \"image\"},)\n",
        "\n",
        "    dataset_discriminator = A.Compose([\n",
        "            A.Resize(width=img_size_discriminator, height=img_size_discriminator),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Normalize(mean=[0.5 for _ in range(channels)], std=[0.5 for _ in range(channels)], max_pixel_value=255),\n",
        "            ToTensorV2(),\n",
        "        ],additional_targets={\"image0\": \"image\"},)\n",
        "\n",
        "\n",
        "    train_path = \".\"\n",
        "    Adults_train = os.path.join(train_path, 'Kids')\n",
        "    Kids_train = os.path.join(train_path, 'Adults')\n",
        "\n",
        "    dataset = TFGAN_Dataset(Adults_train, Kids_train, transform1=dataset_full, transform2=dataset_discriminator,gray_scale=False)\n",
        "    loader = DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS,pin_memory=True,)\n",
        "\n",
        "    #Loss functions\n",
        "    L1 = nn.L1Loss()\n",
        "    mse = nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "    disc_Adults_down = Discriminador().to(DEVICE)\n",
        "    disc_Kids_down = Discriminador().to(DEVICE)\n",
        "    down_sampler = Generator_TFGan_Up_Modelo_1(img_size).to(DEVICE)\n",
        "\n",
        "    opt_disc_Adults_down = optim.Adam(disc_Adults_down.parameters(),\n",
        "                               lr=LEARNING_RATE_DIS,betas=(0.5, 0.999),)\n",
        "    opt_disc_Kids_down = optim.Adam(disc_Kids_down.parameters(),\n",
        "                               lr=LEARNING_RATE_DIS,betas=(0.5, 0.999),)\n",
        "    opt_gen_down = optim.Adam(down_sampler.parameters(),\n",
        "                              lr=LEARNING_RATE_GEN,betas=(0.5, 0.999),)\n",
        "    d_down_scaler = torch.cuda.amp.GradScaler()\n",
        "    g_down_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #debug\n",
        "    print(down_sampler)\n",
        "    x = torch.randn((1,channels,img_size,img_size)).to(DEVICE)\n",
        "    print(down_sampler(x).shape)\n",
        "    #TensorBoard\n",
        "    writer = SummaryWriter()\n",
        "    i = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "        print(f\"Currently in epoch: {epoch+1}\")\n",
        "        writer.add_scalar(\"Epoch\", epoch, i)\n",
        "        train_individual_down(\n",
        "            writer, epoch, i,\n",
        "            loader, L1, mse,\n",
        "            disc_Adults_down, disc_Kids_down, down_sampler,\n",
        "            opt_disc_Adults_down, opt_disc_Kids_down,\n",
        "            opt_gen_down, d_down_scaler, g_down_scaler,\n",
        "        )\n",
        "        i+=1\n",
        "    writer.close()\n",
        "\n",
        "    return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq0YmH3om1Ga"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}